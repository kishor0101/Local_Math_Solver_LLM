{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install lib"
      ],
      "metadata": {
        "id": "XuK_sfmMi8a4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pBZ1lGkCStdi"
      },
      "outputs": [],
      "source": [
        "!pip install -q llama-cpp-python huggingface-hub gradio sympy scipy pulp numpy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: Imports"
      ],
      "metadata": {
        "id": "9t9KKU-9jC2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Essential libraries\n",
        "import gradio as gr                     # beautiful chat UI\n",
        "from huggingface_hub import hf_hub_download  # GGUF model download\n",
        "from llama_cpp import Llama             # local LLM inference\n",
        "import sympy as sp                      # symbolic mathematics\n",
        "from sympy import symbols, sympify, solve, diff, integrate, latex, Matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import re                               # expression cleaning\n",
        "import json                             # not used yet, but future-proof\n",
        "from scipy.optimize import minimize     # numerical optimization\n",
        "from pulp import LpProblem, LpMinimize, LpVariable, lpSum, value"
      ],
      "metadata": {
        "id": "fv0Y77cfjF1V"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: Model Download & Load\n",
        "Original - seedha Qwen2.5-1.5B-Q5_K_M load kiya"
      ],
      "metadata": {
        "id": "dL7MEo3RjO-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load lightweight local math-capable LLM\n",
        "MODEL_REPO = \"Qwen/Qwen2.5-1.5B-Instruct-GGUF\"\n",
        "MODEL_FILENAME = \"qwen2.5-1.5b-instruct-q5_k_m.gguf\"\n",
        "\n",
        "print(\"Downloading model from Hugging Face (first time may take a while)...\")\n",
        "model_path = hf_hub_download(repo_id=MODEL_REPO, filename=MODEL_FILENAME)\n",
        "\n",
        "print(\"Initializing llama.cpp backend...\")\n",
        "math_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=0,           # CPU only (safe for most laptops/Colab free)\n",
        "    n_ctx=8192,               # long enough for math reasoning\n",
        "    verbose=False\n",
        ")\n",
        "print(\"Math LLM ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTNAY_CnjWx-",
        "outputId": "6f957722-eaed-48b5-a01c-88e9d4bd738d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model from Hugging Face (first time may take a while)...\n",
            "Initializing llama.cpp backend...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Math LLM ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: Expression Cleaner (preprocess_math_expression)\n",
        "Original - regex heavy tha"
      ],
      "metadata": {
        "id": "dQguv6ljjh2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_math_input(raw_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Safely converts natural language math input into valid SymPy expression.\n",
        "    Removes command words, handles implicit multiplication, fixes power notation.\n",
        "    \"\"\"\n",
        "    text = raw_text.strip().lower()\n",
        "\n",
        "    # Remove instruction words\n",
        "    text = re.sub(r'\\b(solve|find|calculate|what is|integrate|differentiate|derivative|plot|graph|show)\\b\\s*', '', text, flags=re.I)\n",
        "\n",
        "    # Remove trailing equals or =0\n",
        "    text = re.sub(r'\\s*(=|\\bequals?\\b|=0\\s*$).*', '', text, flags=re.I)\n",
        "\n",
        "    # Power: ^ → **\n",
        "    text = text.replace('^', '**')\n",
        "\n",
        "    # Implicit multiplication: 2x → 2*x, xy → x*y, 3π → 3*π\n",
        "    text = re.sub(r'(\\d)([a-zA-Zπ])', r'\\1*\\2', text)\n",
        "    text = re.sub(r'([a-zA-Zπ])(\\d)', r'\\1*\\2', text)\n",
        "    text = re.sub(r'([a-zA-Z])([a-zA-Z])', r'\\1*\\2', text)\n",
        "\n",
        "    # Fraction safety: 1/2x → (1/2)*x\n",
        "    text = re.sub(r'(\\d+)/(\\d+)([a-zA-Z])', r'(\\1/\\2)*\\3', text)\n",
        "\n",
        "    # Common symbols\n",
        "    text = text.replace('sqrt', 'sqrt').replace('pi', 'pi').replace('e', 'E')\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "T8YZom0Cji8a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: MathSolver Class (sabse important part)\n",
        "Original - MathSolver class tha\n"
      ],
      "metadata": {
        "id": "CvWz0-LxkIBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IntelligentMathAssistant:\n",
        "    \"\"\"Handles both symbolic math (SymPy) and natural language explanation (LLM).\"\"\"\n",
        "\n",
        "    def __init__(self, llm_engine):\n",
        "        self.engine = llm_engine\n",
        "        self.conversation_log = []  # store problem + answer pairs\n",
        "\n",
        "    def detect_problem_category(self, query: str) -> str:\n",
        "        \"\"\"Ask LLM to classify the math question type very quickly.\"\"\"\n",
        "        classification_prompt = f\"\"\"Classify this math question into ONE word only:\n",
        "equation | derivative | integral | optimization | matrix | graph | puzzle | other\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        response = self.engine(classification_prompt, max_tokens=20, temperature=0.05)\n",
        "        category = response['choices'][0]['text'].strip().lower()\n",
        "        return category\n",
        "\n",
        "    def try_symbolic_computation(self, raw_query: str, category: str):\n",
        "        \"\"\"Attempt exact symbolic solution using SymPy.\"\"\"\n",
        "        try:\n",
        "            expr_str = clean_math_input(raw_query)\n",
        "            x, y, z = sp.symbols('x y z')\n",
        "            env = {\n",
        "                'sin': sp.sin, 'cos': sp.cos, 'tan': sp.tan,\n",
        "                'exp': sp.exp, 'ln': sp.ln, 'log': sp.log,\n",
        "                'sqrt': sp.sqrt, 'pi': sp.pi, 'E': sp.E\n",
        "            }\n",
        "            expression = sp.sympify(expr_str, locals=env)\n",
        "\n",
        "            if category == 'equation':\n",
        "                solutions = sp.solve(expression, x)\n",
        "                return solutions, \", \".join(str(s) for s in solutions)\n",
        "            elif category == 'derivative':\n",
        "                result = sp.diff(expression, x)\n",
        "                return result, str(result)\n",
        "            elif category == 'integral':\n",
        "                result = sp.integrate(expression, x)\n",
        "                return result, f\"{result} + C\"\n",
        "            elif category == 'optimization':\n",
        "                f = sp.lambdify(x, expression)\n",
        "                res = minimize(f, x0=0)\n",
        "                return res.x[0], f\"Minimum found at x = {res.x[0]:.4f}, value = {res.fun:.4f}\"\n",
        "            elif category == 'matrix':\n",
        "                # Very simple matrix handling\n",
        "                matrix_match = re.search(r'\\[\\[.*?\\]\\]', expr_str)\n",
        "                if matrix_match:\n",
        "                    mat_data = eval(matrix_match.group())\n",
        "                    M = Matrix(mat_data)\n",
        "                    if 'det' in raw_query.lower():\n",
        "                        return M.det(), str(M.det())\n",
        "                    if 'inv' in raw_query.lower():\n",
        "                        return M.inv(), latex(M.inv())\n",
        "            return None, \"Could not solve symbolically\"\n",
        "        except Exception as exc:\n",
        "            return None, f\"Symbolic computation failed: {str(exc)}\"\n",
        "\n",
        "    def generate_human_like_steps(self, question: str, symbolic_result: str = None):\n",
        "        \"\"\"Generate short, textbook-style step-by-step solution using LLM.\"\"\"\n",
        "        base_prompt = \"\"\"Follow this exact style — short clear steps, no chit-chat, end with boxed final answer.\n",
        "\n",
        "Example 1: 2^x + 2*2^x = 24\n",
        "→ 3*2^x = 24\n",
        "→ 2^x = 8\n",
        "→ 2^x = 2^3\n",
        "→ x = 3\n",
        "\\\\boxed{3}\n",
        "\n",
        "Example 2: Sides 5 and 12, find hypotenuse\n",
        "→ c = √(5² + 12²)\n",
        "→ c = √(25 + 144) = √169\n",
        "→ c = 13\n",
        "\\\\boxed{13}\n",
        "\n",
        "Example 3: Derivative of x^x\n",
        "→ y = x^x = e^(x ln x)\n",
        "→ dy/dx = e^(x ln x) * (ln x + 1)\n",
        "→ dy/dx = x^x (ln x + 1)\n",
        "\\\\boxed{x^{x} (\\\\ln x + 1)}\n",
        "\n",
        "Now solve:\n",
        "\"\"\"\n",
        "        if symbolic_result:\n",
        "            base_prompt += f\"Symbolic result to use: {symbolic_result}\\n\\n\"\n",
        "        base_prompt += f\"Question: {question}\\n\\nAnswer (3-7 lines max):\"\n",
        "\n",
        "        response = self.engine(base_prompt, max_tokens=400, temperature=0.15)\n",
        "        return response['choices'][0]['text'].strip()\n",
        "\n",
        "    def generate_graph_if_needed(self, query: str):\n",
        "        \"\"\"Plot expression if it looks like a graphing request.\"\"\"\n",
        "        if not any(word in query.lower() for word in ['plot', 'graph', 'draw', 'curve']):\n",
        "            return None\n",
        "        try:\n",
        "            expr_text = clean_math_input(query)\n",
        "            expr = sp.sympify(expr_text)\n",
        "            f = sp.lambdify(sp.symbols('x'), expr, 'numpy')\n",
        "            x = np.linspace(-10, 10, 500)\n",
        "            y = f(x)\n",
        "            plt.figure(figsize=(7, 4.5))\n",
        "            plt.plot(x, y, color='teal', linewidth=2.5, label=str(expr))\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.axhline(0, color='black', lw=0.8)\n",
        "            plt.axvline(0, color='black', lw=0.8)\n",
        "            plt.title(f\"Graph of {expr}\")\n",
        "            plt.xlabel(\"x\")\n",
        "            plt.ylabel(\"y\")\n",
        "            buf = BytesIO()\n",
        "            plt.savefig(buf, format='png', bbox_inches='tight')\n",
        "            buf.seek(0)\n",
        "            plt.close()\n",
        "            return Image.open(buf)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def process_question(self, question: str):\n",
        "        category = self.detect_problem_category(question)\n",
        "        sym_result, sym_text = self.try_symbolic_computation(question, category)\n",
        "        explanation = self.generate_human_like_steps(question, sym_text if sym_result else None)\n",
        "        graph = self.generate_graph_if_needed(question)\n",
        "        self.conversation_log.append({\"question\": question, \"response\": explanation})\n",
        "        return explanation, graph"
      ],
      "metadata": {
        "id": "IQD5v0WWkKXv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 5: Gradio Chat Interface"
      ],
      "metadata": {
        "id": "SkAwFhjbkQX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the assistant AFTER model is loaded\n",
        "solver = IntelligentMathAssistant(math_llm)\n",
        "\n",
        "def chat_respond(message, history):\n",
        "    if not message.strip():\n",
        "        return \"\", history, None\n",
        "\n",
        "    answer, plot_img = solver.process_question(message)\n",
        "\n",
        "    # Better LaTeX handling for Gradio Markdown\n",
        "    formatted_answer = answer.replace('\\\\(', '$').replace('\\\\)', '$') \\\n",
        "                             .replace('\\\\[', '$$').replace('\\\\]', '$$') \\\n",
        "                             .replace('\\\\boxed{', '**\\\\boxed{')  # bold boxed\n",
        "\n",
        "    history = history + [[message, formatted_answer]]\n",
        "    return \"\", history, plot_img\n",
        "\n",
        "with gr.Blocks(css=\".gradio-container {background-color: #f0f4f8;}\") as demo:\n",
        "    gr.Markdown(\"# Upgraded Math Solver Chat\\nConcise steps • LaTeX + Boxed answers • Plots when needed\")\n",
        "\n",
        "    chatbot = gr.Chatbot(height=550, show_copy_button=True)\n",
        "    textbox = gr.Textbox(placeholder=\"Ask anything math-related...\", label=\"Your question\")\n",
        "    image_out = gr.Image(label=\"Graph (if generated)\", height=350)\n",
        "\n",
        "    textbox.submit(\n",
        "        chat_respond,\n",
        "        inputs=[textbox, chatbot],\n",
        "        outputs=[textbox, chatbot, image_out]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, debug=True)  # debug=True shows errors in UI"
      ],
      "metadata": {
        "id": "EJWeFjdaLg3l"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}